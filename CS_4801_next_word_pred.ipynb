{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAcPXKx20S1I"
      },
      "outputs": [],
      "source": [
        "#Import all necessary libraries\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Access the specific file in Google Drive\n",
        "\n",
        "file_path = '/content/drive/My Drive/Sherlock Holmes.txt'\n",
        "\n",
        "# Check if the file exists and read its content\n",
        "try:\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "        print(\"File content:\\n\", content[:500])  # Display first 500 characters\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found at path: {file_path}\")"
      ],
      "metadata": {
        "id": "4bpQxvmM-FP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "996c9d63-b048-42e3-94fa-9105f0ab7789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File content:\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "                        THE ADVENTURES OF SHERLOCK HOLMES\n",
            "\n",
            "                               Arthur Conan Doyle\n",
            "\n",
            "\n",
            "\n",
            "                                Table of contents\n",
            "\n",
            "               A Scandal in Bohemia\n",
            "               The Red-Headed League\n",
            "               A Case of Identity\n",
            "               The Boscombe Valley Mystery\n",
            "               The Five Orange Pips\n",
            "               The Man with the Twisted Lip\n",
            "               The Adventure of the Blue Carbuncle\n",
            "               The Adventure of the Speckled Band\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing\n"
      ],
      "metadata": {
        "id": "9yuURjDGORs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splits the content string into lines based on newline characters and stores it in lines list\n",
        "lines = content.split('\\n')"
      ],
      "metadata": {
        "id": "8H66T2DoH60g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combines all lines of text from lines list into the data string variable, seperated by a space\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ' '.join(lines)\n",
        "\n",
        "#Cleans that data by removing unnecessary characters such as new lines, carriage returns, byte order marks, and double quotes\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('\"', '').replace('\"', '')"
      ],
      "metadata": {
        "id": "GUnMhC8ZOY_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splits the text into individual words, then rejoins them into a single string with spaces between them\n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "\n",
        "#Display the first 1000 characters in the cleaned data\n",
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "kXUXUA63QFr8",
        "outputId": "1b640c7a-7d07-4636-d984-f35592519e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"THE ADVENTURES OF SHERLOCK HOLMES Arthur Conan Doyle Table of contents A Scandal in Bohemia The Red-Headed League A Case of Identity The Boscombe Valley Mystery The Five Orange Pips The Man with the Twisted Lip The Adventure of the Blue Carbuncle The Adventure of the Speckled Band The Adventure of the Engineer's Thumb The Adventure of the Noble Bachelor The Adventure of the Beryl Coronet The Adventure of the Copper Beeches A SCANDAL IN BOHEMIA Table of contents Chapter 1 Chapter 2 Chapter 3 CHAPTER I To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "AmM-uFvD1TVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates a tokenizer object to convert text into numerical sequences for the model\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#Analyzes the data and builds a vocabulary of unique words/tokens found in the text\n",
        "tokenizer.fit_on_texts([data])"
      ],
      "metadata": {
        "id": "sx2zRSl31eJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saves the trained tokenizer so we can load it later without having to retrain it\n",
        "pickle.dump(tokenizer, open('token.pk1', 'wb'))"
      ],
      "metadata": {
        "id": "luxu_HdVC6vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting to Sequence of Numbers"
      ],
      "metadata": {
        "id": "C-IMSW17HeN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converts the cleaned text data and converts it into a sequence of numbers\n",
        "#Each number represents a specific word from the voacbulary and stores it in the sequence_data variable\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]"
      ],
      "metadata": {
        "id": "Oo9BvcWnEPJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequence_data[:10])\n",
        "\n",
        "print(len(sequence_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb3euO44G2m2",
        "outputId": "c687c8f6-3764-47a6-b4fc-8de0298abb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1561, 5, 129, 34, 647, 4498, 4499, 226, 5]\n",
            "105879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Determines the total number of unique words in the text\n",
        "#Add 1 to the length to accommodate a reserved index\n",
        "vocabSize = len(tokenizer.word_index)+1\n",
        "vocabSize"
      ],
      "metadata": {
        "id": "wflXh4S78j4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e5ed1f-2706-48fd-f886-74f9c3c44974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8200"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting the Data"
      ],
      "metadata": {
        "id": "DjznCcqTEkGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Takes the words and breaks it down into overlapping sequences of 4 words each\n",
        "#The sequences are stored in the list\n",
        "#The end goal is to create sequences of four words to be fed into the model\n",
        "sequence = []\n",
        "for i in range(3, len(sequence_data)):\n",
        "  words = sequence_data[i-3:i+1]\n",
        "  sequence.append(words)"
      ],
      "metadata": {
        "id": "Z9wN4FSqDjIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sequence))\n",
        "\n",
        "sequence = np.array(sequence)\n",
        "print(sequence)"
      ],
      "metadata": {
        "id": "gr7kSkj7EmvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790274d2-8e73-4432-af2e-55cfe0c5228a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105876\n",
            "[[   1 1561    5  129]\n",
            " [1561    5  129   34]\n",
            " [   5  129   34  647]\n",
            " ...\n",
            " [  28    1 8198 8199]\n",
            " [   1 8198 8199 3187]\n",
            " [8198 8199 3187 3186]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide into the independent (input data) and dependent features (output data)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "#loop through sequence to prepare features for model training\n",
        "for i in sequence:\n",
        "  #pass the first 3 data values into the x list\n",
        "  x.append(i[0:3])\n",
        "  #predict the last data value and add to the y list\n",
        "  y.append(i[3])"
      ],
      "metadata": {
        "id": "bfHCTiFJ4E4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the lists into np arrays to make data compatible with ML libraries\n",
        "x = np.array(x)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "hOvKsBWk6N7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the y array to categorical data to help with classification\n",
        "y = to_categorical(y, num_classes=vocabSize)"
      ],
      "metadata": {
        "id": "IgF81G6i62e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the LSTM model"
      ],
      "metadata": {
        "id": "vQNlnBke70mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "#pass the embedding layer using vocab size for input_dim and 10 for output_dim\n",
        "#maps the input words\n",
        "model.add(Embedding(input_dim = vocabSize, output_dim = 10, input_length = len(sequence)))\n",
        "\n",
        "#add 1st LSTM layer having 1000 parameters and return sequences as a memory\n",
        "#retains memory for subsequent layers\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "\n",
        "#add the 2nd LSTM model with 1000 parameters\n",
        "#summarizes the sequence info\n",
        "model.add(LSTM(1000))\n",
        "\n",
        "#add a dense layer to make the data 0s and 1s\n",
        "#utilizes the relu function to learn complex patterns\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "\n",
        "#add final layer to output probabilities for each vocabulary class\n",
        "#uses softmax activation because the data is multi-class\n",
        "model.add(Dense(vocabSize, activation=\"softmax\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48uZGe7C73yj",
        "outputId": "87ec0a75-f651-4184-fdca-b7526823593b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#explicitly initialize the model using build()\n",
        "model.build(input_shape=(None, 3))\n",
        "#display architecture and parameters of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Q9Vk9DR1-NEF",
        "outputId": "782523f6-e3f3-4f73-8f78-d1370f0a2eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m10\u001b[0m)               │          \u001b[38;5;34m82,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1000\u001b[0m)             │       \u001b[38;5;34m4,044,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)                │       \u001b[38;5;34m8,004,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)                │       \u001b[38;5;34m1,001,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8200\u001b[0m)                │       \u001b[38;5;34m8,208,200\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">82,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,044,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,004,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,001,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8200</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,208,200</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,339,200\u001b[0m (81.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,339,200</span> (81.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,339,200\u001b[0m (81.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,339,200</span> (81.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "r-FxkEu-DGMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the modelcheckpoint callback class. saves the model during training\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#create a model checkpoint instance using the file. It also monitors the loss\n",
        "#monitors loss and saves only the best model to prevent overwriting\n",
        "checkpoint = ModelCheckpoint(\"Sherlock Holmes.keras\", monitor=\"loss\", verbose=1, save_best_only=True)\n",
        "\n",
        "#compile the model with categorical crossentropy for the loss function, multi-class classification\n",
        "#use Adam optimizer, an optimization algorithm that adjusts LR dynamically during training\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer = Adam(learning_rate = .001))\n",
        "\n",
        "#train the model on the x and y, for 2 epochs\n",
        "model.fit(x, y, epochs = 2, batch_size=64, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "BG2KDX0uDI6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7742654f-f667-4a5a-e02d-43cd8d6a1bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m1655/1655\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718ms/step - loss: 6.6745\n",
            "Epoch 1: loss improved from inf to 6.36580, saving model to Sherlock Holmes.keras\n",
            "\u001b[1m1655/1655\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1193s\u001b[0m 719ms/step - loss: 6.6743\n",
            "Epoch 2/2\n",
            "\u001b[1m1655/1655\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703ms/step - loss: 5.8114\n",
            "Epoch 2: loss improved from 6.36580 to 5.78783, saving model to Sherlock Holmes.keras\n",
            "\u001b[1m1655/1655\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1201s\u001b[0m 707ms/step - loss: 5.8114\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b038103e6e0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   https://sourestdeeds.github.io/pdf/Deep%20Learning%20with%20Python.pdf\n",
        "*   https://www.tensorflow.org/guide/keras\n",
        "*   https://docs.python.org/3/library/pickle.html\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NwHZ7LuCSLlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('Sherlock Holmes.keras')\n",
        "tokenizer = pickle.load(open('token.pk1', 'rb'))\n",
        "\n",
        "def predict_word(model, tokenizer, text):\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "  for key, value in tokenizer.word_index.items():\n",
        "    if value == preds:\n",
        "      predicted_word = key\n",
        "      break\n",
        "  print(predicted_word)\n",
        "  return predicted_word\n",
        "\n",
        "predict_word(model, tokenizer, \"Once in a\")\n"
      ],
      "metadata": {
        "id": "BX5nYcnO_NoI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "913f82d5-8127-4fe0-ef30-bc5e6ed1fcf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step\n",
            "little\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'little'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of predict_word function\n",
        "\n",
        "Logical Flow: The code is designed to load a trained model and tokenizer, then predict the next word in a text sequence based on the input.\n",
        "Functionality: It uses a neural network model trained on text data (e.g., \"Sherlock Holmes\") to generate predictions.\n",
        "\n",
        "Usability: The function predict_word is modular and can be reused with different inputs.\n",
        "\n",
        "Does the Code Make Sense?\n",
        "\n",
        "The core logic is sound: It loads a trained model and tokenizer, processes the input text, predicts the next word's class index, and maps it back to a word.\n",
        "Issue: predict_classes is deprecated and will throw an error in TensorFlow 2.6+ versions. The corrected prediction line should be:\n",
        "\n",
        "preds = np.argmax(model.predict(sequence), axis=-1)\n",
        "\n",
        "Once updated for compatibility, the code is valid and functional for its intended purpose."
      ],
      "metadata": {
        "id": "K2mB7DSH4KUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Load the previously saved trained model\n",
        "# 'Sherlock Holmes.keras' contains the trained neural network for text prediction.\n",
        "model = load_model('Sherlock Holmes.keras')\n",
        "\n",
        "# Load the tokenizer object that was saved earlier using pickle\n",
        "# The tokenizer is used to encode and decode text into numerical sequences.\n",
        "tokenizer = pickle.load(open('token.pk1', 'rb'))\n",
        "\n",
        "def predict_word(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "    Predict the next word in a sequence based on the trained model and tokenizer.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained Keras sequential model used for predictions.\n",
        "    - tokenizer: Tokenizer object for encoding/decoding text.\n",
        "    - text: Input text string to predict the next word.\n",
        "\n",
        "    Returns:\n",
        "    - str: Predicted next word in the sequence.\n",
        "    \"\"\"\n",
        "    # Convert the input text into a numerical sequence using the tokenizer\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    \n",
        "    # Ensure the sequence is in NumPy array format for compatibility with the model\n",
        "    sequence = np.array(sequence)\n",
        "    \n",
        "    # Predict the class index of the next word using the model\n",
        "    # Note: `predict_classes` is deprecated in newer TensorFlow versions.\n",
        "    preds = np.argmax(model.predict(sequence), axis=-1)\n",
        "    \n",
        "    # Initialize an empty string to store the predicted word\n",
        "    predicted_word = \"\"\n",
        "    \n",
        "    # Search through the tokenizer's word-to-index mapping to find the predicted word\n",
        "    for key, value in tokenizer.word_index.items():\n",
        "        if value == preds:  # If the index matches the prediction\n",
        "            predicted_word = key  # Assign the word to `predicted_word`\n",
        "            break  # Exit the loop once the word is found\n",
        "    \n",
        "    # Print the predicted word\n",
        "    print(predicted_word)\n",
        "    \n",
        "    # Return the predicted word\n",
        "    return predicted_word\n",
        "\n",
        "# Call the function to predict the next word after the phrase \"A Case of\"\n",
        "predict_word(model, tokenizer, \"A Case of\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "88k_C-ZAFT_f"
      }
    }
  ]
}